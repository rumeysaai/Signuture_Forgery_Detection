{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Signature Forgery Detection - Google Colab\n",
        "\n",
        "This notebook trains CNN and Siamese Network models for signature forgery detection using Contrastive Loss.\n",
        "\n",
        "## Features\n",
        "- **CNN Model**: Binary classification for signature verification\n",
        "- **Siamese Network**: Pair-based verification with Contrastive Loss\n",
        "- **Data Augmentation**: Enhanced augmentation for better generalization\n",
        "- **Model Export**: ONNX and OpenVINO export support\n",
        "\n",
        "## Setup Instructions\n",
        "1. Run all cells in order\n",
        "2. Upload your dataset (genuine/ and forged/ folders)\n",
        "3. Train models\n",
        "4. Evaluate and visualize results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install tensorflow>=2.10.0 numpy>=1.21.0 matplotlib>=3.5.0 opencv-python>=4.5.0 scikit-learn>=1.0.0 pillow>=9.0.0 seaborn>=0.11.0 tf2onnx>=1.14.0 -q\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive, files\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive (optional - if you want to use data from Drive)\n",
        "# Uncomment the line below if you want to use data from Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "print(\"‚úÖ Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup Project Structure\n",
        "\n",
        "Create necessary directories for the project.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create project structure\n",
        "os.makedirs('src', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Project structure created!\")\n",
        "print(\"üìÅ Directories: src/, models/, results/, data/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 2: Clone Repository or Upload Source Files\n",
        "\n",
        "**Option A: Clone from GitHub (Recommended)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository from GitHub\n",
        "!git clone https://github.com/rumeysaai/Signuture_Forgery_Detection.git temp_repo\n",
        "!cp -r temp_repo/src/* /content/src/\n",
        "!rm -rf temp_repo\n",
        "\n",
        "# Add src to Python path\n",
        "sys.path.insert(0, '/content/src')\n",
        "\n",
        "print(\"‚úÖ Repository cloned and source files copied!\")\n",
        "print(\"üìÇ Source files available in /content/src/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Option B: Manual Upload (If GitHub clone doesn't work)**\n",
        "\n",
        "If the clone above doesn't work, manually upload these files:\n",
        "- `src/models.py`\n",
        "- `src/train.py`\n",
        "- `src/utils.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual upload (uncomment if needed)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# \n",
        "# # Move uploaded files to src/\n",
        "# for filename in uploaded.keys():\n",
        "#     if filename.endswith('.py'):\n",
        "#         shutil.move(filename, f'src/{filename}')\n",
        "#         print(f\"Moved {filename} to src/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Upload Dataset\n",
        "\n",
        "Upload your dataset. The dataset should contain:\n",
        "- `genuine/` folder with genuine signature images\n",
        "- `forged/` folder with forged signature images\n",
        "\n",
        "**Option A: Upload ZIP file (Recommended)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload ZIP file containing genuine/ and forged/ folders\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract if ZIP file\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('data')\n",
        "        print(f\"‚úÖ Extracted {filename} to data/\")\n",
        "        # Check structure\n",
        "        if os.path.exists('data/genuine') and os.path.exists('data/forged'):\n",
        "            genuine_count = len([f for f in os.listdir('data/genuine') if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "            forged_count = len([f for f in os.listdir('data/forged') if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "            print(f\"üìä Found {genuine_count} genuine signatures\")\n",
        "            print(f\"üìä Found {forged_count} forged signatures\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Warning: genuine/ or forged/ folders not found in ZIP\")\n",
        "    else:\n",
        "        print(f\"üìÅ Uploaded {filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Option B: Use Google Drive**\n",
        "\n",
        "If your dataset is in Google Drive, mount it and copy the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive and copy data\n",
        "# Uncomment and modify the path to your dataset in Drive\n",
        "# drive.mount('/content/drive')\n",
        "# !cp -r '/content/drive/MyDrive/your_dataset_path/*' /content/data/\n",
        "# print(\"‚úÖ Data copied from Google Drive!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Verify Setup\n",
        "\n",
        "Check that all required files and data are in place.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify source files\n",
        "required_files = ['models.py', 'train.py', 'utils.py']\n",
        "missing_files = []\n",
        "\n",
        "for file in required_files:\n",
        "    if os.path.exists(f'src/{file}'):\n",
        "        print(f\"‚úÖ {file} found\")\n",
        "    else:\n",
        "        print(f\"‚ùå {file} missing\")\n",
        "        missing_files.append(file)\n",
        "\n",
        "# Verify data structure\n",
        "if os.path.exists('data/genuine') and os.path.exists('data/forged'):\n",
        "    genuine_count = len([f for f in os.listdir('data/genuine') if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "    forged_count = len([f for f in os.listdir('data/forged') if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "    print(f\"\\n‚úÖ Dataset structure correct\")\n",
        "    print(f\"   Genuine signatures: {genuine_count}\")\n",
        "    print(f\"   Forged signatures: {forged_count}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Dataset structure incorrect. Please ensure data/genuine/ and data/forged/ exist\")\n",
        "\n",
        "if not missing_files:\n",
        "    print(\"\\nüéâ Setup complete! Ready to train models.\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è Please upload missing files: {missing_files}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Train Siamese Network with Contrastive Loss\n",
        "\n",
        "Train the Siamese Network model using Contrastive Loss for signature verification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "from train import train_siamese_model\n",
        "import sys\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, '/content/src')\n",
        "\n",
        "# Training parameters\n",
        "data_directory = \"data\"\n",
        "epochs = 30\n",
        "batch_size = 8\n",
        "learning_rate = 0.001\n",
        "img_size = (128, 128)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SIAMESE NETWORK TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Data directory: {data_directory}\")\n",
        "print(f\"Epochs: {epochs}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Loss function: Contrastive Loss (margin=1.0, threshold=0.5)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train model\n",
        "siamese_model, siamese_history, siamese_metrics = train_siamese_model(\n",
        "    data_directory,\n",
        "    model_save_path='models/siamese_model.h5',\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    learning_rate=learning_rate,\n",
        "    img_size=img_size\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING COMPLETED!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model saved to: models/siamese_model.h5\")\n",
        "print(f\"Embedding network saved to: models/siamese_embedding.h5\")\n",
        "print(f\"\\nFinal Metrics:\")\n",
        "print(f\"  Accuracy: {siamese_metrics['accuracy']:.4f}\")\n",
        "print(f\"  Precision: {siamese_metrics['precision']:.4f}\")\n",
        "print(f\"  Recall: {siamese_metrics['recall']:.4f}\")\n",
        "print(f\"  F1-Score: {siamese_metrics['f1_score']:.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display training history plot\n",
        "from IPython.display import Image, display\n",
        "\n",
        "if os.path.exists('results/siamese_training_history.png'):\n",
        "    display(Image('results/siamese_training_history.png'))\n",
        "else:\n",
        "    print(\"Training history plot not found\")\n",
        "\n",
        "# Display confusion matrix\n",
        "if os.path.exists('results/siamese_confusion_matrix.png'):\n",
        "    display(Image('results/siamese_confusion_matrix.png'))\n",
        "else:\n",
        "    print(\"Confusion matrix not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Download Models and Results\n",
        "\n",
        "Download trained models and results to your local machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download trained models\n",
        "from google.colab import files\n",
        "\n",
        "# Download Siamese model\n",
        "if os.path.exists('models/siamese_model.h5'):\n",
        "    files.download('models/siamese_model.h5')\n",
        "    print(\"‚úÖ Downloaded siamese_model.h5\")\n",
        "\n",
        "# Download embedding network\n",
        "if os.path.exists('models/siamese_embedding.h5'):\n",
        "    files.download('models/siamese_embedding.h5')\n",
        "    print(\"‚úÖ Downloaded siamese_embedding.h5\")\n",
        "\n",
        "# Download results\n",
        "if os.path.exists('results/siamese_metrics.json'):\n",
        "    files.download('results/siamese_metrics.json')\n",
        "    print(\"‚úÖ Downloaded siamese_metrics.json\")\n",
        "\n",
        "if os.path.exists('results/siamese_confusion_matrix.png'):\n",
        "    files.download('results/siamese_confusion_matrix.png')\n",
        "    print(\"‚úÖ Downloaded siamese_confusion_matrix.png\")\n",
        "\n",
        "if os.path.exists('results/siamese_training_history.png'):\n",
        "    files.download('results/siamese_training_history.png')\n",
        "    print(\"‚úÖ Downloaded siamese_training_history.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Test Model Inference (Optional)\n",
        "\n",
        "Test the trained model on sample images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained model and test on sample images\n",
        "from tensorflow import keras\n",
        "from models import create_siamese_network_with_contrastive, compile_siamese_with_contrastive\n",
        "from utils import prepare_dataset\n",
        "import numpy as np\n",
        "\n",
        "# Load model\n",
        "print(\"Loading trained model...\")\n",
        "siamese_model = keras.models.load_model('models/siamese_model.h5')\n",
        "embedding_model = keras.models.load_model('models/siamese_embedding.h5')\n",
        "\n",
        "# Load test data\n",
        "print(\"Loading test data...\")\n",
        "X, y = prepare_dataset('data', (128, 128))\n",
        "X_test = X[-100:]  # Use last 100 images for testing\n",
        "y_test = y[-100:]\n",
        "\n",
        "# Test on a few pairs\n",
        "from models import prepare_siamese_pairs\n",
        "test_pairs_a, test_pairs_b, test_labels = prepare_siamese_pairs(X_test, y_test, num_pairs=10)\n",
        "\n",
        "# Predict\n",
        "distances = siamese_model.predict([test_pairs_a, test_pairs_b], verbose=0)\n",
        "predictions = (distances < 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(\"=\" * 60)\n",
        "for i in range(min(10, len(predictions))):\n",
        "    actual = \"Same\" if test_labels[i] == 1 else \"Different\"\n",
        "    predicted = \"Same\" if predictions[i] == 1 else \"Different\"\n",
        "    distance = distances[i][0]\n",
        "    correct = \"‚úÖ\" if test_labels[i] == predictions[i] else \"‚ùå\"\n",
        "    print(f\"Pair {i+1}: Distance={distance:.4f}, Actual={actual}, Predicted={predicted} {correct}\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
